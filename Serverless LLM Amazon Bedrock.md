# Serverless LLM Amazon Bedrock (DeepLearning.AI Tutorial)

Learn how to deploy a large language model-based application into production using serverless technology.

* Learn how to prompt and customize your LLM responses using Amazon Bedrock.
* Summarize audio conversations by first transcribing an audio file and passing the transcription to an LLM.
* Deploy an event-driven audio summarizer that runs as new audio files are uploaded; using a serverless architecture.

References:

**Documentation Link:** [Amazon Bedrock User Guide](https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-bedrock.html)
**Generative AI Code Samples for Amazon Bedrock:** [Link](https://community.aws/code/generative-ai)
**Community Blogposts:** [AWS Community Generative AI](https://community.aws/generative-ai)

[TOC]

***

## Traditional Server Architectures

* Server Management - Provisioning, Config, and maintainence of servers (physical or virtual) including operating system updates, patching, capacity planing and scaling.

* Always On: Servers run continuously, incurring costs when they're not being used.

* Scaling Limitations: Manually scaling up or down to adjust compute or storage capacity can be time-consuming.

* Coupled to Infrastructure: Applications are often tightly bound to the underlying server infrastructure, making migrations complex.


## Serverless Architectures

* Simplified Management: Serverless platforms (like AWS Lambda, Azure Functions, etc.) handle infrastructure provisioning, scaling, and maintenance, allowing developers to focus on code.

* Cost Efficiency: You pay only for the resources used during execution, making it cost-effective for low to moderate workloads.

* Automatic Scaling: Serverless platforms automatically scale your application based on demand, ensuring optimal performance without manual intervention.

* Rapid Development: Developers can focus on writing code without worrying about server management, leading to faster development and deployment cycles.

* Event-Driven: Ideal for event-driven architectures, such as real-time data processing or responding to triggers like HTTP requests or data changes.

***

## Components for Serverless LLM Application Deployment using Amazon Bedrock

* Serverless Functions: Core pieces of your application logic deployed as individual functions (e.g., AWS Lambda) that are triggered by events and scale automatically.

* Amazon Bedrock: AWS service to access and integrate a variety of large language models (LLMs) from supported providers for text generation, translation, summarization, and more.

* Event Triggers: Mechanisms that initiate your serverless functions. Common examples include new files on an Amazon S3 bucket, or HTTP requests to an API Gateway

* Data Storage: Services like Amazon S3 to store data needed by or generated by your application.

* API Gateway (Optional): An AWS service to create and manage RESTful APIs to allow external applications to interact with your LLM functionality.

***

## 1. First Generation with Amazon Bedrock

### Setting up Bedrock Runtime:

```python
## IMPORTING REQUIRED PACKAGES
import boto3 						# AWS SDK for Python
import json 						# JSON for data handling

## SETUP BEDROCK RUNTIME
bedrock_runtime = boto3.client('bedrock-runtime', region_name = 'us-west-2')
prompt = "Write a one sentence summary of Las Vegas."

# Defining keyword arguments for the required service
# Specify media type for request and response bodies in the `contentType` and `accept` fields
kwargs = {
    "modelId": "amazon.titan-text-lite-v1", # Bedrock LLM Model
    "contentType": "application/json",		# Default: application/json for both fields
    "accept": "*/*",						# Accepts any response type
    "body": json.dumps(
        {
            "inputText": prompt				# Wrap the prompt in a JSON string
        }
    )
}
```

### Running Inference:

```python
# Run inference on model through the API by sending an InvokeModel request with given inputs
response = bedrock_runtime.invoke_model(**kwargs)
response 													# streaming body response output
response_body = json.loads(response.get('body').read())		# Decode and print full JSON response in pretty-printed format
print(json.dumps(response_body, indent=4))

# Print only the summarized text output i.e. generation
print(response_body['results'][0]['outputText'])
```

### Generation Configuration

```python

kwargs = {
    "modelId": "amazon.titan-text-express-v1",
    "contentType": "application/json",
    "accept": "*/*",
    "body" : json.dumps(
        {
            "inputText": prompt,
            # Titan models support the following inference parameters for text models:
            "textGenerationConfig": {
                "maxTokenCount": 100, 	# Limit maximum number of tokens in generated response
                "temperature": 0.7, 	# Use a lower value to decrease randomness in response 
                "topP": 0.9				# Use a lower value to ignore less probable options
            }
        }
    )
}
# Note: Check completion reason for responses

	# FINISH - Response was fully generated
	# LENGTH - Response was truncated because of the response length set (maxTokenCount)

```

### Working with Other Types of Data (e.g. Audio Files and Text Files)

```python
# Loading audio file
from IPython.display import Audio
audio = Audio(filename="dialog.mp3")
display(audio)

# Read contents of transcript txt file
with open('transcript.txt', "r") as file:
    dialogue_text = file.read()
print(dialogue_text)

```

### Prompt Template for Working with Audio Files

```python
# Prepare a prompt to get summary of the dialogue within the transcript
# Note the prompt engineering structure:
prompt = f"""The text between the <transcript> XML tags is a transcript of a conversation. 
Write a short summary of the conversation.

<transcript>
{dialogue_text}
</transcript>

Here is a summary of the conversation in the transcript:"""

# Specify configurations for calling LLM on Bedrock
kwargs = {
    "modelId": "amazon.titan-text-express-v1",
    "contentType": "application/json",
    "accept": "*/*",
    "body": json.dumps(
        {
            "inputText": prompt,
            "textGenerationConfig": {
                "maxTokenCount": 512,
                "temperature": 0,
                "topP": 0.9
            }
        }
    )
}
```
***

## 2. Summarizing an Audio File

Transcribing audio files and using LLMs to analyze. Outline:

1. Import Packages and Loading Audio File
2. Setup: S3 Client and Transcribe Client
3. Uploading audio file to S3, creating unique job name and building transcription response
4. Setup Bedrock Runtime, create prompt template and configure model response
5. Generate summary of the audio transcript

### Importing Required Packages

```python
import os			# For working with environment variables
from IPython.display import Audio
import boto3		# Python AWS SDK
import uuid4		# Creating unique identifiers (for transcription job)
import time   		# Check transcription status at timed interval
import json         # Working with JSON data
from jinja2 import Template  # For creating prompt template
```

### Transcribing Audio File using AWS Transcribe

```python
audio = Audio(filename="dialog.mp3")
display(audio)

# Setup S3 client for file management
s3_client = boto3.client('s3', region_name='us-west-2')
bucket_name = os.environ['BucketName'] # Environment variable with S3 bucket name
file_name = 'dialog.mp3'
s3_client.upload_file(file_name, bucket_name, file_name) # Upload to S3

# Setup Transcribe client for audio -> text
transcribe_client = boto3.client('transcribe', region_name='us-west-2')

# Unique job name for AWS transcribe
job_name = 'transcription-job-' + str(uuid.uuid4())
job_name

# Start transcription job (pointer to the ongoing job)
# ... configure details (audio location, options)
response = transcribe_client.start_transcription_job(
    TranscriptionJobName=job_name,
    Media={'MediaFileUri': f's3://{bucket_name}/{file_name}'}, # specify media location in bucket
    MediaFormat='mp3',
    LanguageCode='en-US',
    OutputBucketName=bucket_name,	# Save transcript back to S3 bucket
    Settings={
        'ShowSpeakerLabels': True, # Attempt to identify different speakers
        'MaxSpeakerLabels': 2 # Able to identify maximum of 10 speakers
    }
)

# Wait for transcription to complete (check status every 2 seconds)
while True:
    status = transcribe_client.get_transcription_job(TranscriptionJobName=job_name)
    if status['TranscriptionJob']['TranscriptionJobStatus'] in ['COMPLETED', 'FAILED']:
        break
    time.sleep(2)
print(status['TranscriptionJob']['TranscriptionJobStatus'])

```

### Processing Completed Transcript

```python
if status['TranscriptionJob']['TranscriptionJobStatus'] == 'COMPLETED':
    
    # Load the transcript JSON from S3
    transcript_key = f"{job_name}.json" # Construct filename using job name
    transcript_obj = s3_client.get_object(Bucket=bucket_name, Key=transcript_key)
    transcript_text = transcript_obj['Body'].read().decode('utf-8') # Read raw JSON content of transcript and decode as text
    transcript_json = json.loads(transcript_text) # Load into Python dictionary for easier handling
    
    # print(transcript_json)

    # Construct text transcript (formatting speaker labels)
    output_text = ""				# Initialize empty string to build transcript
    current_speaker = None			# Variable to track the current speaker
    
    items = transcript_json['results']['items']
    
    # Logic to structure transcript text:
    # Here, you'd iterate through the 'items', identify speaker changes and format the text accordingly
    for item in items:
        
        speaker_label = item.get('speaker_label', None)
        content = item['alternatives'][0]['content']
        
        # print(f"{speaker_label}: {content}")

        # Start the line with the speaker label
        if speaker_label is not None and speaker_label != current_speaker:
            current_speaker = speaker_label
            output_text += f"\n{current_speaker}: "
            
        # Add the speech content
        if item['type'] == 'punctuation':
            output_text = output_text.rstrip()
            
        output_text += f"{content} "
        
    print(output_text)

    # Save the formatted transcript text
    with open(f'{job_name}.txt', 'w') as f:
        f.write(output_text)

```

### Using LLM for Generating Summary of Transcript

```python
bedrock_runtime = boto3.client('bedrock-runtime', region_name='us-west-2')
with open(f'{job_name}.txt', "r") as file: 				# Read transcript file
    transcript = file.read()
```

For production cases (e.g. Serverless Cloud Architectures), define template in a seperate file - able to apply version control specifically to the template. Seperates prompt from the application code. 

* `%%writefile`: Syntax specific to Jupyter Notebooks. It writes the prompt content into a file named `prompt_template.txt`.

The following prompt template instructs LLM to produce a structured JSON output with sentiment analysis and issue identification:

```python
%%writefile prompt_template.txt
I need to summarize a conversation. The transcript of the 
conversation is between the <data> XML like tags.

<data>
{{transcript}}
</data>

The summary must contain a one word sentiment analysis, and 
a list of issues, problems or causes of friction
during the conversation. The output must be provided in 
JSON format shown in the following example. 

Example output:
{
    "sentiment": <sentiment>,
    "issues": [
        {
            "topic": <topic>,
            "summary": <issue_summary>,
        }
    ]
}

Write the JSON output and nothing more.

Here is the JSON output:
```

Setup Bedrock runtime and invoke model based on defined parameters:

```python
with open('prompt_template.txt', "r") as file:
    template_string = file.read()

# Load transcript into data dictionary
data = {
    'transcript' : transcript
}

# Loading prompt template (using Jinja2)
template = Template(template_string)
prompt = template.render(data) # Load actual transcript into template
print(prompt)					# Display constructed prompt

kwargs = {
	# .... Configurations for calling LLM on Bedrock
    "modelId": "amazon.titan-text-express-v1",
    "contentType": "application/json",
    "accept": "*/*",
    "body": json.dumps(
        {
            "inputText": prompt,
            "textGenerationConfig": {
                "maxTokenCount": 512,
                "temperature": 0,		# Note temperature set to zero
                "topP": 0.9
            }
        }
    )
}
response = bedrock_runtime.invoke_model(**kwargs)

# Decode JSON response and extract generated output text
response_body = json.loads(response.get('body').read())
generation = response_body['results'][0]['outputText']
print(generation)

```

***

## 3. Enabling Logging

```python
import boto3
import json
import os

bedrock = boto3.client('bedrock', region_name="us-west-2")
from helpers.CloudWatchHelper import CloudWatch_Helper
cloudwatch = CloudWatch_Helper()

log_group_name = '/my/amazon/bedrock/logs'

cloudwatch.create_log_group(log_group_name)

loggingConfig = {
    'cloudWatchConfig': {
        'logGroupName': log_group_name,
        'roleArn': os.environ['LOGGINGROLEARN'],
        'largeDataDeliveryS3Config': {
            'bucketName': os.environ['LOGGINGBUCKETNAME'],
            'keyPrefix': 'amazon_bedrock_large_data_delivery',
        }
    },
    's3Config': {
        'bucketName': os.environ['LOGGINGBUCKETNAME'],
        'keyPrefix': 'amazon_bedrock_logs',
    },
    'textDataDeliveryEnabled': True,
}

bedrock.put_model_invocation_logging_configuration(loggingConfig=loggingConfig)
bedrock.get_model_invocation_logging_configuration()

bedrock_runtime = boto3.client('bedrock-runtime', region_name="us-west-2")
prompt = "Write an article about the fictional planet Foobar."
kwargs = {
    "modelId": "amazon.titan-text-express-v1",
    "contentType": "application/json",
    "accept": "*/*",
    "body": json.dumps(
        {
            "inputText": prompt,
            "textGenerationConfig": {
                "maxTokenCount": 512,
                "temperature": 0.7,
                "topP": 0.9
            }
        }
    )
}

response = bedrock_runtime.invoke_model(**kwargs)
response_body = json.loads(response.get('body').read())

generation = response_body['results'][0]['outputText']
print(generation)

cloudwatch.print_recent_logs(log_group_name)

from IPython.display import HTML
aws_url = os.environ['AWS_CONSOLE_URL']

HTML(f'<a href="{aws_url}" target="_blank">GO TO AWS CONSOLE</a>')

```

***

## 4. Deploy an AWS Lambda function

1. Setup - Import packages and access helpers functions
2. Build the prompt template
3. Create Lambda function - Lambda handler, extract transcript from text, summarization using Bedrock
4. Deploy Lambda function
5. Performing testing

```python

```

***

## 5. Event-Driven Generation

### Importing Required Packages

```python
import boto3, os

from helpers.Lambda_Helper import Lambda_Helper
from helpers.S3_Helper import S3_Helper

lambda_helper = Lambda_Helper()
s3_helper = S3_Helper()

bucket_name_text = os.environ['LEARNERS3BUCKETNAMETEXT']
bucket_name_audio = os.environ['LEARNERS3BUCKETNAMEAUDIO']
```

### Deploying Lambda Function

```python
%%writefile lambda_function.py

#############################################################
#
# This Lambda function is written to a file by the notebook 
# It does not run in the notebook!
#
#############################################################

import json
import boto3
import uuid
import os

s3_client = boto3.client('s3')
transcribe_client = boto3.client('transcribe', region_name='us-west-2')

def lambda_handler(event, context):
    # Extract the bucket name and key from the incoming event
    bucket = event['Records'][0]['s3']['bucket']['name']
    key = event['Records'][0]['s3']['object']['key']

    # One of a few different checks to ensure we don't end up in a recursive loop.
    if key != "dialog.mp3": 
        print("This demo only works with dialog.mp3.")
        return

    try:
        
        job_name = 'transcription-job-' + str(uuid.uuid4()) # Needs to be a unique name

        response = transcribe_client.start_transcription_job(
            TranscriptionJobName=job_name,
            Media={'MediaFileUri': f's3://{bucket}/{key}'},
            MediaFormat='mp3',
            LanguageCode='en-US',
            OutputBucketName= os.environ['S3BUCKETNAMETEXT'],  # specify the output bucket
            OutputKey=f'{job_name}-transcript.json',
            Settings={
                'ShowSpeakerLabels': True,
                'MaxSpeakerLabels': 2
            }
        )
        
    except Exception as e:
        print(f"Error occurred: {e}")
        return {
            'statusCode': 500,
            'body': json.dumps(f"Error occurred: {e}")
        }

    return {
        'statusCode': 200,
        'body': json.dumps(f"Submitted transcription job for {key} from bucket {bucket}.")
    }
```

```python
lambda_helper.lambda_environ_variables = {'S3BUCKETNAMETEXT' : bucket_name_text}
lambda_helper.deploy_function(["lambda_function.py"], function_name="LambdaFunctionTranscribe")

lambda_helper.filter_rules_suffix = "mp3"
lambda_helper.add_lambda_trigger(bucket_name_audio, function_name="LambdaFunctionTranscribe")

s3_helper.upload_file(bucket_name_audio, 'dialog.mp3')
s3_helper.list_objects(bucket_name_audio)
```

**Restart kernel if needed.**

* If you run the code fairly quickly from start to finish, it's possible that the following code cell `s3_helper.list_objects(bucket_name_text)` will give a "Not Found" error.
* If waiting a few seconds (10 seconds) and re-running this cell does not resolve the error, then you can restart the kernel of the jupyter notebook.
* Go to menu->Kernel->Restart Kernel.
* Then run the code cells from the start of the notebook, waiting 2 seconds or so for each code cell to finish executing.

```python
s3_helper.list_objects(bucket_name_text)
```

**Re-run "download" code cell as needed**

* It may take a few seconds for the results to be generated.
* If you see a Not Found error, please wait a few seconds and then try running the `s3_helper.download_object` again.

```python
s3_helper.download_object(bucket_name_text, 'results.txt')

from helpers.Display_Helper import Display_Helper
display_helper = Display_Helper()
display_helper.text_file('results.txt')
```

***

## Concluding Notes








